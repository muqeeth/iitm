{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "from sparsesvd import sparsesvd\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return sps.csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('./temp/wordsim353/combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw json text is in temp directory\n",
    "loaded_data = {}\n",
    "with open('./temp/content0.raw') as json_file:  \n",
    "    loaded_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create index files for articles and words\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "article2ind = {}\n",
    "word2ind = {}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "ind = 0\n",
    "temp1 = 0 \n",
    "for article in loaded_data:\n",
    "    article2ind[article] = ind\n",
    "    ind = ind + 1\n",
    "    tokens  = word_tokenize(loaded_data[article]['text'])\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "#         tokens = [stem(token) for token in tokens]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "    tokens = [porter.stem(token) for token in tokens]\n",
    "    for token in tokens:\n",
    "        if token not in word2ind.keys():\n",
    "            word2ind[token] = temp1\n",
    "            temp1 = temp1 + 1\n",
    "                \n",
    "with open('./temp/article2ind.json','w') as outfile:\n",
    "    json.dump(article2ind,outfile)\n",
    "with open('./temp/word2ind.json','w') as outfile:\n",
    "    json.dump(word2ind,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for article, doc in loaded_data.items():\n",
    "    tokens  = word_tokenize(doc['text'])\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "#         tokens = [stem(token) for token in tokens]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "    tokens = [porter.stem(token) for token in tokens]\n",
    "    wordmap = Counter(tokens).items()\n",
    "    for word,count in wordmap:\n",
    "        if word in word_count:\n",
    "            word_count[word] +=1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "with open('./temp/word_count.json','w') as outfile:\n",
    "    json.dump(word_count,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_articles = len(article2ind)\n",
    "n_words = len(word2ind)\n",
    "mat = sps.dok_matrix((n_words, n_articles), dtype=np.float32)\n",
    "for article, doc in loaded_data.items():\n",
    "    j = article2ind[article]\n",
    "    tokens  = word_tokenize(doc['text'])\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "#         tokens = [porter.stem(token) for token in tokens]\n",
    "#         tokens = [stem(token) for token in tokens]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "    tokens = [porter.stem(token) for token in tokens]\n",
    "    wordmap = Counter(tokens).items()\n",
    "    for word, count in wordmap:\n",
    "        i = word2ind[word]\n",
    "        mat[i,j] = count*np.log(n_articles/word_count[word])\n",
    "mat = mat.tocsr()\n",
    "save_sparse_csr(\"./temp/mat.npz\", mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esa= load_sparse_csr(\"./temp/mat.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind = {}\n",
    "with open('./temp/word2ind.json') as json_file:  \n",
    "    word2ind = json.load(json_file)\n",
    "esa_sim = []  \n",
    "word_sim = []\n",
    "lemma = WordNetLemmatizer()\n",
    "for index, row in df1.iterrows():\n",
    "    word1 = lemma.lemmatize(row[\"Word 1\"].lower())\n",
    "    word2 = lemma.lemmatize(row[\"Word 2\"].lower())\n",
    "    try:\n",
    "        i = word2ind[word1]\n",
    "        j = word2ind[word2]\n",
    "    except:\n",
    "        continue\n",
    "    sim = np.dot(esa[i,:],esa[j,:].T)/(np.sqrt((np.dot(esa[i,:],esa[i,:].T))*\n",
    "                                               (np.dot(esa[j,:],esa[j,:].T))))\n",
    "    esa_sim.append(float(sim))\n",
    "    word_sim.append(row[\"Human (mean)\"])\n",
    "corr, p_value = spearmanr(esa_sim, word_sim)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smat = sps.csc_matrix(esa.T)\n",
    "ut, s, vt = sparsesvd(smat, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_sim = []  \n",
    "word_sim = []\n",
    "lemma = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "for index, row in df1.iterrows():\n",
    "    word1 = lemma.lemmatize(row[\"Word 1\"].lower())\n",
    "    word2 = lemma.lemmatize(row[\"Word 2\"].lower())\n",
    "    word1 = porter.stem(word1)\n",
    "    word2 = porter.stem(word2)\n",
    "    try:\n",
    "        i = word2ind[word1]\n",
    "        j = word2ind[word2]\n",
    "    except:\n",
    "        continue\n",
    "    l = np.multiply(vt.T[i],s)\n",
    "    l = np.matmul(l,ut)\n",
    "    r = np.multiply(vt.T[j],s)\n",
    "    r = np.matmul(r,ut)\n",
    "    sim = np.dot(l,r.T)/np.sqrt(np.dot(l,l.T)*np.dot(r,r.T))\n",
    "    lsa_sim.append(float(sim))\n",
    "    word_sim.append(row[\"Human (mean)\"])\n",
    "corr, p_value = spearmanr(lsa_sim, word_sim)\n",
    "print(corr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import imgkit\n",
    "syn = pd.read_csv('./temp/synonyms_esa.csv')\n",
    "hyp = pd.read_csv('./temp/hypernyms_esa.csv')\n",
    "ant = pd.read_csv('./temp/antonmys_esa.csv')\n",
    "mer = pd.read_csv('./temp/meronyms_esa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['syn','hyp','ant','mer']\n",
    "for i,df in enumerate([syn,hyp,ant,mer]):\n",
    "    html = df.style.background_gradient(cmap='RdBu',axis = 0).render()\n",
    "    imgkit.from_string(html,names[i]+'_esa.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = pd.read_csv('./temp/synonyms_lsa.csv')\n",
    "hyp = pd.read_csv('./temp/hypernyms_lsa.csv')\n",
    "ant = pd.read_csv('./temp/antonmys_lsa.csv')\n",
    "mer = pd.read_csv('./temp/meronyms_lsa.csv')\n",
    "names = ['syn','hyp','ant','mer']\n",
    "for i,df in enumerate([syn,hyp,ant,mer]):\n",
    "    html = df.style.background_gradient(cmap='RdBu',axis = 0).render()\n",
    "    imgkit.from_string(html,names[i]+'_lsa.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

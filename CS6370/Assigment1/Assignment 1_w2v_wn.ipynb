{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:03:44.254374Z",
     "start_time": "2019-03-14T08:03:35.878607Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim,logging\n",
    "import seaborn as sns\n",
    "import imgkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:10:03.537402Z",
     "start_time": "2019-03-14T08:10:03.504613Z"
    }
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv('wordsim353/combined.csv',usecols=[0,1,2])\n",
    "df2=pd.read_csv('wordsim353/set1.csv',usecols=[0,1,2])\n",
    "df3=pd.read_csv('wordsim353/set2.csv',usecols=[0,1,2])\n",
    "syn=pd.read_csv('synonyms.csv')\n",
    "hyp=pd.read_csv('hypernyms.csv')\n",
    "ant=pd.read_csv('antonmys.csv')\n",
    "mer=pd.read_csv('meronyms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:04:15.654516Z",
     "start_time": "2019-03-14T08:03:44.332320Z"
    }
   },
   "outputs": [],
   "source": [
    "def lc(x):\n",
    "    sims=[0]\n",
    "    for syn1 in wn.synsets(x[1]):\n",
    "        for syn2 in wn.synsets(x[0]):\n",
    "            if syn1.pos()=='n' and syn2.pos()=='n':\n",
    "                #depth=max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())\n",
    "                depth = 20\n",
    "                dist=syn1.shortest_path_distance(syn2)\n",
    "                sim=-math.log((dist + 1) / (2.0 * depth))\n",
    "                sims.append(sim)\n",
    "    return max(sims)\n",
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['lc']=df.apply(lc,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:04:23.912654Z",
     "start_time": "2019-03-14T08:04:15.660809Z"
    }
   },
   "outputs": [],
   "source": [
    "def wp(x):\n",
    "    sims=[0]\n",
    "    for syn1 in wn.synsets(x[0]):\n",
    "        for syn2 in wn.synsets(x[1]):\n",
    "            lcs= syn2.lowest_common_hypernyms(syn1,use_min_depth=True)\n",
    "            if len(lcs)==0:\n",
    "                sims.append(0)\n",
    "            else:\n",
    "                try:\n",
    "                    lc=lcs[0]\n",
    "                    n3=lc.max_depth()\n",
    "                    n1,n2=syn1.shortest_path_distance(lc),syn2.shortest_path_distance(lc)\n",
    "                    sim=2.*n3/(n1+n2+2.*n3)\n",
    "                    sims.append(sim)\n",
    "                except: pass\n",
    "    return max(sims)\n",
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['wp']=df.apply(wp,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:04:27.149947Z",
     "start_time": "2019-03-14T08:04:23.918348Z"
    }
   },
   "outputs": [],
   "source": [
    "def liu(x):\n",
    "    sims=[0]\n",
    "    for syn1 in wn.synsets(x[0]):\n",
    "        for syn2 in wn.synsets(x[1]):\n",
    "            lcs= syn2.lowest_common_hypernyms(syn1,use_min_depth=True)\n",
    "            if len(lcs)==0:\n",
    "                sims.append(0)\n",
    "            else:\n",
    "                a,b=1,100\n",
    "                lc=lcs[0]\n",
    "                H=lc.max_depth()\n",
    "                L=syn1.shortest_path_distance(syn2)\n",
    "                sim=math.exp(-a*L)*(1-math.exp(-2*b*H))/(1+math.exp(-2*b*H))\n",
    "                sims.append(sim)\n",
    "    return max(sims)\n",
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['liu']=df.apply(liu,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:04:29.414479Z",
     "start_time": "2019-03-14T08:04:27.155424Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "wiki=pd.read_csv('wiki_data.csv')\n",
    "corpus =wiki.text.replace('', np.nan).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:04:59.273132Z",
     "start_time": "2019-03-14T08:04:29.421037Z"
    }
   },
   "outputs": [],
   "source": [
    "words=pd.concat([df1['Word 1'],df1['Word 2'],\n",
    "                 df2['Word 1'],df2['Word 2'],\n",
    "                 df3['Word 1'],df3['Word 2'],\n",
    "                 syn['Word 1'],syn['Word 2'],\n",
    "                 hyp['Word 1'],hyp['Word 2'],\n",
    "                 ant['Word 1'],ant['Word 2'],\n",
    "                 mer['Word 1'],mer['Word 2']]).unique()\n",
    "glosses={}\n",
    "for word in words:\n",
    "    gloss=''\n",
    "    for sn in wn.synsets(word):\n",
    "        defi=sn.definition()\n",
    "        gloss+= ' ' + defi\n",
    "    glosses[word]=gloss\n",
    "\n",
    "text=''\n",
    "for key in glosses: text+= ' ' + glosses[key]\n",
    "vocab=set(nltk.word_tokenize(text))\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:05:31.324614Z",
     "start_time": "2019-03-14T08:04:59.279154Z"
    }
   },
   "outputs": [],
   "source": [
    "vectors={}\n",
    "\n",
    "for key in glosses:\n",
    "    words=nltk.word_tokenize(glosses[key])\n",
    "    idx=map(lambda x: vectorizer.vocabulary_[x],words)\n",
    "    vectors[key]=np.array(X[:,idx].sum(1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:05:33.418756Z",
     "start_time": "2019-03-14T08:05:31.336822Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['lesk']=df.apply(lambda x: np.corrcoef(vectors[x[0]],vectors[x[1]])[1,0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T04:16:35.376510Z",
     "start_time": "2019-03-14T04:16:35.056872Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.corr(method='spearman').iloc[0,1:].plot()\n",
    "df2.corr(method='spearman').iloc[0,1:].plot()\n",
    "df3.corr(method='spearman').iloc[0,1:].plot()\n",
    "plt.legend(['combined','set1','set2'])\n",
    "plt.savefig('wn_corr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T23:05:46.404229Z",
     "start_time": "2019-03-13T23:05:38.823402Z"
    }
   },
   "outputs": [],
   "source": [
    "names=['syn','hyp','ant','mer']\n",
    "for i,df in enumerate([syn,hyp,ant,mer]):\n",
    "    html=df.style.background_gradient(cmap='RdBu',axis=0).render()\n",
    "    imgkit.from_string(html, names[i]+'_wn.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T04:22:40.946390Z",
     "start_time": "2019-03-14T04:22:40.934786Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab=words=pd.concat([df1['Word 1'],df1['Word 2']]).unique()\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T06:57:32.247202Z",
     "start_time": "2019-03-14T06:55:11.839216Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "sentences=reuters.sents()\n",
    "i=0\n",
    "\n",
    "i+=2\n",
    "model = gensim.models.Word2Vec(sentences, min_count=5,iter=2,size=100,sg=1)\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(model.wv.vectors[:10000,:].T,cmap='RdBu', vmin=-1, vmax=1,aspect=50)\n",
    "plt.savefig('w2v_sg_i'+str(i))\n",
    "#for df in [syn,hyp,ant,mer]:\n",
    "#    df['w2v_'+str(i)]=df.apply(helper,axis=1)\n",
    "\n",
    "\n",
    "for k in range(5):\n",
    "    i+=2\n",
    "    model.train(sentences,epochs=2,total_examples=model.corpus_count)\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.imshow(model.wv.vectors[:10000,:].T,cmap='RdBu', vmin=-1, vmax=1,aspect=50)\n",
    "    plt.savefig('w2v_sg_i'+str(i))\n",
    "    #for df in [syn,hyp,ant,mer]:\n",
    "    #    df['w2v_'+str(i)]=df.apply(lambda x:,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T08:13:51.639971Z",
     "start_time": "2019-03-14T08:13:46.084021Z"
    }
   },
   "outputs": [],
   "source": [
    "def helper(x):\n",
    "    try:return model_wiki.similarity(x[0],x[1])\n",
    "    except: return np.nan\n",
    "\n",
    "wiki=pd.read_csv('wiki_data.csv')\n",
    "wiki_sent=list(wiki.text.str.split(' '))\n",
    "wiki_sent=filter(lambda x: type(x)==list,wiki_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T07:14:45.358606Z",
     "start_time": "2019-03-14T07:08:13.135731Z"
    }
   },
   "outputs": [],
   "source": [
    "i=2\n",
    "model_wiki = gensim.models.Word2Vec(wiki_sent, min_count=2,iter=2,size=100,sg=1)\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(model_wiki.wv.vectors[:10000,:].T,cmap='RdBu', vmin=-1, vmax=1,aspect=50)\n",
    "#plt.savefig('w2v_i'+str(i))\n",
    "for df in [syn,hyp,ant,mer]:\n",
    "    df['w2v_sg_'+str(i)]=df.apply(helper,axis=1)\n",
    "\n",
    "\n",
    "for k in range(2):\n",
    "    i+=2\n",
    "    model_wiki.train(wiki_sent,epochs=2,total_examples=model_wiki.corpus_count)\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.imshow(model_wiki.wv.vectors[:10000,:].T,cmap='RdBu', vmin=-1, vmax=1,aspect=50)\n",
    "    #plt.savefig('w2v_i'+str(i))\n",
    "    for df in [syn,hyp,ant,mer]:\n",
    "        df['w2v_sg_'+str(i)]=df.apply(helper,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T09:35:14.306656Z",
     "start_time": "2019-03-14T09:35:14.292758Z"
    }
   },
   "outputs": [],
   "source": [
    "def helper(x):\n",
    "    try:return model.similarity(x[0].lower(),x[1].lower())\n",
    "    except: return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T09:50:28.510401Z",
     "start_time": "2019-03-14T09:35:59.360127Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(wiki_sent, min_count=2,iter=5,size=100,sg=0)\n",
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['100_cbow']=df.apply(helper,axis=1)\n",
    "print 1\n",
    "model = gensim.models.Word2Vec(wiki_sent, min_count=2,iter=5,size=50,sg=0)\n",
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['50_cbow']=df.apply(helper,axis=1)\n",
    "print 2\n",
    "model = gensim.models.Word2Vec(wiki_sent, min_count=2,iter=5,size=100,sg=1)\n",
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['100_sg']=df.apply(helper,axis=1)\n",
    "print 3\n",
    "model = gensim.models.Word2Vec(wiki_sent, min_count=2,iter=5,size=50,sg=1)\n",
    "for df in [df1,df2,df3,syn,hyp,ant,mer]:\n",
    "    df['50_sg']=df.apply(helper,axis=1)\n",
    "print 4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T09:56:42.779115Z",
     "start_time": "2019-03-14T09:56:42.244596Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.corr(method='spearman').iloc[0,1:].plot()\n",
    "df2.corr(method='spearman').iloc[0,1:].plot()\n",
    "df3.corr(method='spearman').iloc[0,1:].plot()\n",
    "plt.legend(['combined','set1','set2'])\n",
    "plt.savefig('w2v_corr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:05:06.334361Z",
     "start_time": "2019-03-14T13:05:06.313844Z"
    }
   },
   "outputs": [],
   "source": [
    "print df1.corr(method='spearman').iloc[0,1:]\n",
    "print df2.corr(method='spearman').iloc[0,1:]\n",
    "print df3.corr(method='spearman').iloc[0,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T09:55:10.535894Z",
     "start_time": "2019-03-14T09:55:01.939071Z"
    }
   },
   "outputs": [],
   "source": [
    "names=['syn','hyp','ant','mer']\n",
    "for i,df in enumerate([syn,hyp,ant,mer]):\n",
    "    html=df.style.background_gradient(cmap='RdBu').render()\n",
    "    imgkit.from_string(html, names[i]+'_w2v.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T07:14:53.167920Z",
     "start_time": "2019-03-14T07:14:45.363556Z"
    }
   },
   "outputs": [],
   "source": [
    "names=['syn','hyp','ant','mer']\n",
    "for i,df in enumerate([syn,hyp,ant,mer]):\n",
    "    html=df.style.background_gradient(cmap='RdBu').render()\n",
    "    imgkit.from_string(html, names[i]+'_w2v_sg_epochs.png')\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
